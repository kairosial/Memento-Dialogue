# FastAPI AI ì„œë¹„ìŠ¤ êµ¬í˜„ ìƒì„¸ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-08-18 17:45  
**ì—°ê´€ ë¬¸ì„œ**: [250818_1745-ì•„í‚¤í…ì²˜ ê°œì„  ë°©í–¥ ë° êµ¬í˜„ ê³„íš.md]  
**ëª©ì **: AI ì„œë¹„ìŠ¤ í†µí•©ì„ ìœ„í•œ êµ¬ì²´ì  êµ¬í˜„ ë°©ë²• ì œì‹œ

---

## ğŸ—ï¸ êµ¬í˜„ ë‹¨ê³„ë³„ ìƒì„¸ ê°€ì´ë“œ

### Step 1: í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì¶”ê°€

#### 1.1 Poetry ì˜ì¡´ì„± ì¶”ê°€
```bash
cd backend
poetry add langchain==0.1.20
poetry add langchain-openai==0.1.8
poetry add langgraph==0.1.5
poetry add openai==1.35.0
poetry add tiktoken==0.7.0
poetry add pydantic-ai==0.0.12
```

#### 1.2 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
# backend/.env
# ê¸°ì¡´ ë³€ìˆ˜ ìœ ì§€
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_key
JWT_SECRET_KEY=your_jwt_secret

# ìƒˆë¡œ ì¶”ê°€
OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-4o-mini
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langchain_api_key  # ì„ íƒì‚¬í•­
```

#### 1.3 ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
```python
# backend/app/core/config.py
class Settings(BaseSettings):
    # ê¸°ì¡´ ì„¤ì • ìœ ì§€
    app_name: str = "Memento Box API"
    # ... ê¸°ì¡´ ì„¤ì •ë“¤ ...
    
    # AI ì„œë¹„ìŠ¤ ì„¤ì • ì¶”ê°€
    openai_api_key: str
    openai_model: str = "gpt-4o-mini"
    max_tokens: int = 1000
    temperature: float = 0.7
    
    # LangChain ì„¤ì • (ì„ íƒì‚¬í•­)
    langchain_tracing_v2: bool = False
    langchain_api_key: Optional[str] = None
```

---

### Step 2: AI ì„œë¹„ìŠ¤ ê¸°ë³¸ êµ¬ì¡° êµ¬í˜„

#### 2.1 ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
```bash
mkdir -p backend/app/services
mkdir -p backend/app/agents
mkdir -p backend/app/prompts
touch backend/app/services/__init__.py
touch backend/app/agents/__init__.py
touch backend/app/prompts/__init__.py
```

#### 2.2 ê¸°ë³¸ AI ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
```python
# backend/app/services/ai_service.py
from langchain_openai import ChatOpenAI
from langchain.schema import BaseMessage, HumanMessage, SystemMessage
from typing import List, Dict, Any, Optional
import logging
from ..core.config import settings

logger = logging.getLogger(__name__)

class AIService:
    """ê¸°ë³¸ AI ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=settings.openai_model,
            temperature=settings.temperature,
            max_tokens=settings.max_tokens,
            api_key=settings.openai_api_key
        )
        
    async def generate_response(
        self, 
        messages: List[BaseMessage],
        **kwargs
    ) -> str:
        """AI ì‘ë‹µ ìƒì„±"""
        try:
            response = await self.llm.ainvoke(messages, **kwargs)
            return response.content
        except Exception as e:
            logger.error(f"AI response generation failed: {e}")
            raise
    
    async def analyze_photo(
        self, 
        photo_url: str, 
        user_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‚¬ì§„ ë¶„ì„ ë° ëŒ€í™” ì†Œì¬ ìƒì„±"""
        system_prompt = """
        ë‹¹ì‹ ì€ ì‚¬ì§„ì„ ë¶„ì„í•˜ì—¬ íšŒìƒ ëŒ€í™”ë¥¼ ë„ì™€ì£¼ëŠ” AIì…ë‹ˆë‹¤.
        ì‚¬ì§„ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
        1. ì‚¬ì§„ ì„¤ëª…
        2. ëŒ€í™” ì‹œì‘ ì§ˆë¬¸ 3ê°œ
        3. ê°ì •ì  í†¤ (ê¸ì •ì /ì¤‘ë¦½ì /ì¡°ì‹¬ìŠ¤ëŸ¬ìš´)
        4. ì˜ˆìƒ ê¸°ì–µ ì¹´í…Œê³ ë¦¬ (ê°€ì¡±, ì—¬í–‰, ì¼ìƒ, íŠ¹ë³„í•œ ìˆœê°„ ë“±)
        """
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ì‚¬ì§„ URL: {photo_url}\nì‚¬ìš©ì ì •ë³´: {user_context}")
        ]
        
        response = await self.generate_response(messages)
        
        # TODO: êµ¬ì¡°í™”ëœ ì‘ë‹µ íŒŒì‹± ë¡œì§ êµ¬í˜„
        return {
            "description": "ì‚¬ì§„ ë¶„ì„ ê²°ê³¼",
            "conversation_starters": [],
            "emotional_tone": "positive",
            "memory_category": "general"
        }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
ai_service = AIService()
```

#### 2.3 CIST í‰ê°€ ì„œë¹„ìŠ¤
```python
# backend/app/services/cist_service.py
from typing import Dict, List, Optional
from enum import Enum
import re
from ..models.session import CISTCategory
from .ai_service import ai_service
from langchain.schema import SystemMessage, HumanMessage

class CISTDifficulty(Enum):
    EASY = 1
    MEDIUM = 2
    HARD = 3

class CISTService:
    """CIST ì¸ì§€í‰ê°€ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.ai_service = ai_service
        self.category_prompts = {
            "orientation_time": "ì‹œê°„ ì§€ë‚¨ë ¥ ê´€ë ¨ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
            "orientation_place": "ì¥ì†Œ ì§€ë‚¨ë ¥ ê´€ë ¨ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
            "memory_registration": "ê¸°ì–µ ë“±ë¡ ëŠ¥ë ¥ í‰ê°€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
            "memory_recall": "ê¸°ì–µ íšŒìƒ ëŠ¥ë ¥ í‰ê°€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
            "attention": "ì£¼ì˜ì§‘ì¤‘ë ¥ í‰ê°€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
            "executive_function": "ì‹¤í–‰ê¸°ëŠ¥ í‰ê°€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
            "language_naming": "ì–¸ì–´ëª…ëª… ëŠ¥ë ¥ í‰ê°€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
        }
    
    async def generate_question(
        self, 
        category: str,
        difficulty: int = 1,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """CIST ì¹´í…Œê³ ë¦¬ë³„ ì§ˆë¬¸ ìƒì„±"""
        
        system_prompt = f"""
        ë‹¹ì‹ ì€ CIST(í•œêµ­íŒ ê°„ì´ì¸ì§€ê²€ì‚¬) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        {category} ì¹´í…Œê³ ë¦¬ì˜ ë‚œì´ë„ {difficulty} ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        ì‘ë‹µ í˜•ì‹:
        {{
            "question": "ì§ˆë¬¸ ë‚´ìš©",
            "expected_answer": "ì˜ˆìƒ ì •ë‹µ",
            "scoring_criteria": "ì±„ì  ê¸°ì¤€",
            "max_score": ì ìˆ˜
        }}
        """
        
        category_instruction = self.category_prompts.get(
            category, 
            "ì¸ì§€ëŠ¥ë ¥ í‰ê°€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
        )
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=category_instruction)
        ]
        
        response = await self.ai_service.generate_response(messages)
        
        # TODO: JSON íŒŒì‹± ë° ê²€ì¦ ë¡œì§ êµ¬í˜„
        return {
            "question": "ì˜¤ëŠ˜ì€ ëª‡ ì›” ë©°ì¹ ì…ë‹ˆê¹Œ?",
            "expected_answer": "ì •í™•í•œ ì›”/ì¼",
            "scoring_criteria": "ì •í™•í•˜ë©´ 1ì , í‹€ë¦¬ë©´ 0ì ",
            "max_score": 1
        }
    
    async def evaluate_response(
        self,
        question: str,
        expected_answer: str,
        user_response: str,
        category: str
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì‘ë‹µ í‰ê°€ ë° ì ìˆ˜ ê³„ì‚°"""
        
        system_prompt = f"""
        CIST {category} ì¹´í…Œê³ ë¦¬ ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {question}
        ì˜ˆìƒ ì •ë‹µ: {expected_answer}
        ì‚¬ìš©ì ì‘ë‹µ: {user_response}
        
        í‰ê°€ ê¸°ì¤€:
        1. ì •í™•ë„ (0-1ì )
        2. ë¶€ë¶„ ì ìˆ˜ ê°€ëŠ¥ì„±
        3. í‰ê°€ ê·¼ê±°
        
        ì‘ë‹µ í˜•ì‹:
        {{
            "score": ì ìˆ˜,
            "is_correct": true/false,
            "partial_credit": ë¶€ë¶„ì ìˆ˜,
            "feedback": "í‰ê°€ ê·¼ê±°"
        }}
        """
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content="ìœ„ ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”.")
        ]
        
        response = await self.ai_service.generate_response(messages)
        
        # TODO: êµ¬ì¡°í™”ëœ í‰ê°€ ê²°ê³¼ íŒŒì‹±
        return {
            "score": 1,
            "is_correct": True,
            "partial_credit": 0.0,
            "feedback": "ì •í™•í•œ ì‘ë‹µì…ë‹ˆë‹¤."
        }
    
    async def calculate_total_score(self, session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ë³„ CIST ì´ì  ë° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°"""
        # TODO: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì„¸ì…˜ ì‘ë‹µ ì¡°íšŒ ë° ì§‘ê³„
        return {
            "total_score": 18,
            "max_possible": 21,
            "category_scores": {
                "orientation": 4,
                "memory": 6,
                "attention": 3,
                "executive": 3,
                "language": 2
            },
            "cognitive_status": "normal"
        }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
cist_service = CISTService()
```

#### 2.4 íšŒìƒ ëŒ€í™” ì„œë¹„ìŠ¤ (LangGraph í™œìš©)
```python
# backend/app/services/conversation_service.py
from langgraph import StateGraph, END
from typing import Dict, List, Any, TypedDict
import uuid
from .ai_service import ai_service
from ..core.database import supabase

class ConversationState(TypedDict):
    """ëŒ€í™” ìƒíƒœ ì •ì˜"""
    session_id: str
    user_id: str
    current_photo_id: str
    conversation_history: List[Dict[str, str]]
    current_question: str
    user_response: str
    conversation_stage: str  # "intro", "exploring", "deepening", "closing"
    emotional_tone: str
    next_action: str

class ConversationService:
    """LangGraph ê¸°ë°˜ íšŒìƒ ëŒ€í™” ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.ai_service = ai_service
        self.conversation_graph = self._build_conversation_graph()
    
    def _build_conversation_graph(self) -> StateGraph:
        """ëŒ€í™” íë¦„ ê·¸ë˜í”„ ìƒì„±"""
        workflow = StateGraph(ConversationState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("start_conversation", self._start_conversation)
        workflow.add_node("generate_question", self._generate_question)
        workflow.add_node("process_response", self._process_response)
        workflow.add_node("determine_next", self._determine_next_step)
        workflow.add_node("end_conversation", self._end_conversation)
        
        # ì—£ì§€ ì¶”ê°€
        workflow.set_entry_point("start_conversation")
        workflow.add_edge("start_conversation", "generate_question")
        workflow.add_edge("generate_question", "process_response")
        workflow.add_edge("process_response", "determine_next")
        
        # ì¡°ê±´ë¶€ ì—£ì§€
        workflow.add_conditional_edges(
            "determine_next",
            self._should_continue,
            {
                "continue": "generate_question",
                "end": "end_conversation"
            }
        )
        
        workflow.add_edge("end_conversation", END)
        
        return workflow.compile()
    
    async def _start_conversation(self, state: ConversationState) -> ConversationState:
        """ëŒ€í™” ì‹œì‘"""
        state["conversation_stage"] = "intro"
        state["emotional_tone"] = "warm"
        
        # ì²« ë²ˆì§¸ ì‚¬ì§„ ì •ë³´ ì¡°íšŒ
        photo_result = supabase.table("photos").select("*").eq("id", state["current_photo_id"]).execute()
        if photo_result.data:
            photo = photo_result.data[0]
            # ì‚¬ì§„ ë¶„ì„ì„ í†µí•œ ì´ˆê¸° ì§ˆë¬¸ ìƒì„±
            analysis = await self.ai_service.analyze_photo(
                photo["file_path"], 
                {"user_id": state["user_id"]}
            )
            state["current_question"] = analysis.get("conversation_starters", ["ì´ ì‚¬ì§„ì— ëŒ€í•´ ë§ì”€í•´ ì£¼ì„¸ìš”."])[0]
        
        return state
    
    async def _generate_question(self, state: ConversationState) -> ConversationState:
        """ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±"""
        system_prompt = f"""
        ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ íšŒìƒ ëŒ€í™” ì¹˜ë£Œì‚¬ì…ë‹ˆë‹¤.
        í˜„ì¬ ëŒ€í™” ë‹¨ê³„: {state['conversation_stage']}
        ê°ì •ì  í†¤: {state['emotional_tone']}
        
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ í›„ì† ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì§ˆë¬¸ì€ ìì—°ìŠ¤ëŸ½ê³  ê³µê°ì ì´ì–´ì•¼ í•˜ë©°, ê¸°ì–µì„ ìê·¹í•˜ëŠ” ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        """
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_text = "\n".join([
            f"ì§ˆë¬¸: {conv['question']}\nì‘ë‹µ: {conv['response']}"
            for conv in state["conversation_history"]
        ])
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ëŒ€í™” ë‚´ì—­:\n{history_text}\n\në‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."}
        ]
        
        next_question = await self.ai_service.generate_response(messages)
        state["current_question"] = next_question
        
        return state
    
    async def _process_response(self, state: ConversationState) -> ConversationState:
        """ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬"""
        # ì‘ë‹µì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        state["conversation_history"].append({
            "question": state["current_question"],
            "response": state["user_response"],
            "timestamp": "2025-08-18T17:45:00Z"  # ì‹¤ì œë¡œëŠ” í˜„ì¬ ì‹œê°„
        })
        
        # ê°ì • í†¤ ë¶„ì„ ë° ì¡°ì •
        # TODO: ì‚¬ìš©ì ì‘ë‹µì˜ ê°ì • ìƒíƒœ ë¶„ì„
        
        return state
    
    async def _determine_next_step(self, state: ConversationState) -> ConversationState:
        """ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
        # ëŒ€í™” ê¸¸ì´, ì‚¬ìš©ì ì°¸ì—¬ë„ ë“±ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ í–‰ë™ ê²°ì •
        conversation_length = len(state["conversation_history"])
        
        if conversation_length < 3:
            state["next_action"] = "continue"
            state["conversation_stage"] = "exploring"
        elif conversation_length < 6:
            state["next_action"] = "continue"
            state["conversation_stage"] = "deepening"
        else:
            state["next_action"] = "end"
            state["conversation_stage"] = "closing"
        
        return state
    
    def _should_continue(self, state: ConversationState) -> str:
        """ëŒ€í™” ê³„ì† ì—¬ë¶€ íŒë‹¨"""
        return state["next_action"]
    
    async def _end_conversation(self, state: ConversationState) -> ConversationState:
        """ëŒ€í™” ì¢…ë£Œ"""
        # ëŒ€í™” ì„¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        session_data = {
            "id": state["session_id"],
            "status": "completed",
            "total_duration_seconds": len(state["conversation_history"]) * 120,  # ì¶”ì •ì¹˜
        }
        
        # TODO: ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ êµ¬í˜„
        
        return state
    
    async def start_conversation(
        self,
        user_id: str,
        photo_ids: List[str]
    ) -> str:
        """ìƒˆ íšŒìƒ ëŒ€í™” ì„¸ì…˜ ì‹œì‘"""
        session_id = str(uuid.uuid4())
        
        initial_state: ConversationState = {
            "session_id": session_id,
            "user_id": user_id,
            "current_photo_id": photo_ids[0] if photo_ids else "",
            "conversation_history": [],
            "current_question": "",
            "user_response": "",
            "conversation_stage": "intro",
            "emotional_tone": "warm",
            "next_action": "continue"
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì„¸ì…˜ ìƒì„±
        session_data = {
            "user_id": user_id,
            "session_type": "reminiscence",
            "status": "active",
            "selected_photos": photo_ids
        }
        
        result = supabase.table("sessions").insert(session_data).execute()
        
        return session_id
    
    async def process_user_input(
        self,
        session_id: str,
        user_input: str
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±"""
        
        # í˜„ì¬ ìƒíƒœ ë³µì› (ì‹¤ì œë¡œëŠ” ìºì‹œ/DBì—ì„œ ì¡°íšŒ)
        current_state: ConversationState = {
            "session_id": session_id,
            "user_id": "temp_user",  # ì‹¤ì œë¡œëŠ” ì„¸ì…˜ì—ì„œ ì¡°íšŒ
            "current_photo_id": "temp_photo",
            "conversation_history": [],
            "current_question": "",
            "user_response": user_input,
            "conversation_stage": "exploring",
            "emotional_tone": "warm",
            "next_action": "continue"
        }
        
        # ëŒ€í™” ê·¸ë˜í”„ ì‹¤í–‰
        result = await self.conversation_graph.ainvoke(current_state)
        
        return {
            "next_question": result["current_question"],
            "conversation_stage": result["conversation_stage"],
            "should_continue": result["next_action"] == "continue"
        }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
conversation_service = ConversationService()
```

---

### Step 3: API ë¼ìš°í„° êµ¬í˜„

#### 3.1 AI ë¼ìš°í„° ìƒì„±
```python
# backend/app/routers/ai_router.py
from fastapi import APIRouter, HTTPException, Depends, status
from typing import List, Dict, Any, Optional
from ..services.conversation_service import conversation_service
from ..services.cist_service import cist_service
from ..core.deps import get_current_user_id
from ..models.ai import (  # ìƒˆë¡œ ìƒì„± í•„ìš”
    ConversationStartRequest,
    ConversationResponse,
    CISTQuestionRequest,
    CISTEvaluationRequest,
    AIResponse
)

router = APIRouter(prefix="/ai", tags=["ai"])

# íšŒìƒ ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸
@router.post("/conversations/start", response_model=AIResponse)
async def start_conversation(
    request: ConversationStartRequest,
    current_user_id: str = Depends(get_current_user_id)
):
    """íšŒìƒ ëŒ€í™” ì„¸ì…˜ ì‹œì‘"""
    try:
        session_id = await conversation_service.start_conversation(
            user_id=current_user_id,
            photo_ids=request.photo_ids
        )
        
        return AIResponse(
            success=True,
            message="ëŒ€í™” ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data={"session_id": session_id}
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ëŒ€í™” ì‹œì‘ ì‹¤íŒ¨: {str(e)}"
        )

@router.post("/conversations/{session_id}/respond", response_model=ConversationResponse)
async def process_conversation_response(
    session_id: str,
    user_input: str,
    current_user_id: str = Depends(get_current_user_id)
):
    """ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬ ë° ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±"""
    try:
        result = await conversation_service.process_user_input(
            session_id=session_id,
            user_input=user_input
        )
        
        return ConversationResponse(
            session_id=session_id,
            next_question=result["next_question"],
            conversation_stage=result["conversation_stage"],
            should_continue=result["should_continue"]
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
        )

# CIST í‰ê°€ ì—”ë“œí¬ì¸íŠ¸
@router.post("/cist/question")
async def generate_cist_question(
    request: CISTQuestionRequest,
    current_user_id: str = Depends(get_current_user_id)
):
    """CIST ì§ˆë¬¸ ìƒì„±"""
    try:
        question = await cist_service.generate_question(
            category=request.category,
            difficulty=request.difficulty
        )
        
        return AIResponse(
            success=True,
            message="CIST ì§ˆë¬¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data=question
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        )

@router.post("/cist/evaluate")
async def evaluate_cist_response(
    request: CISTEvaluationRequest,
    current_user_id: str = Depends(get_current_user_id)
):
    """CIST ì‘ë‹µ í‰ê°€"""
    try:
        evaluation = await cist_service.evaluate_response(
            question=request.question,
            expected_answer=request.expected_answer,
            user_response=request.user_response,
            category=request.category
        )
        
        return AIResponse(
            success=True,
            message="ì‘ë‹µì´ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            data=evaluation
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì‘ë‹µ í‰ê°€ ì‹¤íŒ¨: {str(e)}"
        )
```

#### 3.2 AI ëª¨ë¸ ì •ì˜
```python
# backend/app/models/ai.py
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

class ConversationStartRequest(BaseModel):
    photo_ids: List[str]
    session_type: Optional[str] = "reminiscence"

class ConversationResponse(BaseModel):
    session_id: str
    next_question: str
    conversation_stage: str
    should_continue: bool

class CISTQuestionRequest(BaseModel):
    category: str
    difficulty: int = 1
    context: Optional[Dict[str, Any]] = None

class CISTEvaluationRequest(BaseModel):
    question: str
    expected_answer: str
    user_response: str
    category: str

class AIResponse(BaseModel):
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None
```

#### 3.3 ë©”ì¸ ì•±ì— ë¼ìš°í„° ì¶”ê°€
```python
# backend/app/main.py
from .routers import auth_router, users_router, photos_router, sessions_router, ai_router  # ì¶”ê°€

# ë¼ìš°í„° ë“±ë¡
app.include_router(ai_router, prefix=API_V1_PREFIX)  # ì¶”ê°€
```

---

### Step 4: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

#### 4.1 ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```python
# backend/test_ai_services.py
import asyncio
import os
from dotenv import load_dotenv

load_dotenv()

async def test_ai_service():
    """AI ì„œë¹„ìŠ¤ ê¸°ë³¸ í…ŒìŠ¤íŠ¸"""
    from app.services.ai_service import ai_service
    from langchain.schema import HumanMessage
    
    # ê¸°ë³¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸
    messages = [HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”, í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.")]
    response = await ai_service.generate_response(messages)
    print(f"AI ì‘ë‹µ: {response}")

async def test_cist_service():
    """CIST ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸"""
    from app.services.cist_service import cist_service
    
    # ì§ˆë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸
    question = await cist_service.generate_question("orientation_time", 1)
    print(f"ìƒì„±ëœ ì§ˆë¬¸: {question}")
    
    # ì‘ë‹µ í‰ê°€ í…ŒìŠ¤íŠ¸
    evaluation = await cist_service.evaluate_response(
        "ì˜¤ëŠ˜ì€ ëª‡ ì›” ë©°ì¹ ì…ë‹ˆê¹Œ?",
        "8ì›” 18ì¼",
        "8ì›” 18ì¼ì´ìš”",
        "orientation_time"
    )
    print(f"í‰ê°€ ê²°ê³¼: {evaluation}")

if __name__ == "__main__":
    asyncio.run(test_ai_service())
    asyncio.run(test_cist_service())
```

#### 4.2 API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
```bash
# íšŒìƒ ëŒ€í™” ì‹œì‘ í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/v1/ai/conversations/start" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "photo_ids": ["photo-id-1", "photo-id-2"]
  }'

# CIST ì§ˆë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/v1/ai/cist/question" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "category": "orientation_time",
    "difficulty": 1
  }'
```

---

### Step 5: í”„ë¡ íŠ¸ì—”ë“œ í†µí•© ì¤€ë¹„

#### 5.1 API í´ë¼ì´ì–¸íŠ¸ ìˆ˜ì •
```typescript
// frontend/src/lib/api.ts
class APIClient {
  private baseURL = 'http://localhost:8000/api/v1';
  private token: string | null = null;

  setAuthToken(token: string) {
    this.token = token;
  }

  private async request(endpoint: string, options: RequestInit = {}) {
    const url = `${this.baseURL}${endpoint}`;
    const headers = {
      'Content-Type': 'application/json',
      ...(this.token && { Authorization: `Bearer ${this.token}` }),
      ...options.headers,
    };

    const response = await fetch(url, { ...options, headers });
    return response.json();
  }

  // AI ì„œë¹„ìŠ¤ ê´€ë ¨ ë©”ì„œë“œë“¤
  async startConversation(photoIds: string[]) {
    return this.request('/ai/conversations/start', {
      method: 'POST',
      body: JSON.stringify({ photo_ids: photoIds }),
    });
  }

  async respondToConversation(sessionId: string, userInput: string) {
    return this.request(`/ai/conversations/${sessionId}/respond`, {
      method: 'POST',
      body: JSON.stringify({ user_input: userInput }),
    });
  }

  async generateCISTQuestion(category: string, difficulty: number = 1) {
    return this.request('/ai/cist/question', {
      method: 'POST',
      body: JSON.stringify({ category, difficulty }),
    });
  }
}

export const apiClient = new APIClient();
```

#### 5.2 React í›… ìƒì„±
```typescript
// frontend/src/hooks/useConversation.ts
import { useState, useCallback } from 'react';
import { apiClient } from '../lib/api';

export interface ConversationState {
  sessionId: string | null;
  currentQuestion: string;
  conversationHistory: Array<{
    question: string;
    response: string;
  }>;
  isActive: boolean;
  stage: string;
}

export function useConversation() {
  const [conversation, setConversation] = useState<ConversationState>({
    sessionId: null,
    currentQuestion: '',
    conversationHistory: [],
    isActive: false,
    stage: 'idle',
  });

  const startConversation = useCallback(async (photoIds: string[]) => {
    try {
      const response = await apiClient.startConversation(photoIds);
      
      if (response.success) {
        setConversation(prev => ({
          ...prev,
          sessionId: response.data.session_id,
          isActive: true,
          stage: 'intro',
        }));
      }
    } catch (error) {
      console.error('ëŒ€í™” ì‹œì‘ ì‹¤íŒ¨:', error);
    }
  }, []);

  const respondToConversation = useCallback(async (userInput: string) => {
    if (!conversation.sessionId) return;

    try {
      const response = await apiClient.respondToConversation(
        conversation.sessionId,
        userInput
      );

      setConversation(prev => ({
        ...prev,
        currentQuestion: response.next_question,
        conversationHistory: [
          ...prev.conversationHistory,
          { question: prev.currentQuestion, response: userInput }
        ],
        stage: response.conversation_stage,
        isActive: response.should_continue,
      }));
    } catch (error) {
      console.error('ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    }
  }, [conversation.sessionId]);

  return {
    conversation,
    startConversation,
    respondToConversation,
  };
}
```

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

### 1. í™˜ê²½ ì„¤ì •
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
cd backend
poetry install

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ì— OpenAI API í‚¤ ì¶”ê°€
```

### 2. ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
```bash
# AI ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
poetry run python test_ai_services.py
```

### 3. API ì„œë²„ ì‹œì‘
```bash
# ê°œë°œ ì„œë²„ ì‹¤í–‰
poetry run python run_server.py
```

### 4. API ë¬¸ì„œ í™•ì¸
ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8000/docs` ì ‘ì†í•˜ì—¬ ìƒˆë¡œìš´ AI ì—”ë“œí¬ì¸íŠ¸ í™•ì¸

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤
1. **OpenAI API í‚¤ ì˜¤ë¥˜**: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í™•ì¸
2. **ì˜ì¡´ì„± ì¶©ëŒ**: `poetry lock --no-update` ì‹¤í–‰
3. **ë©”ëª¨ë¦¬ ë¶€ì¡±**: LangChain ëª¨ë¸ ìºì‹± ì„¤ì • ì¡°ì •
4. **ì‘ë‹µ ì§€ì—°**: OpenAI API íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¦ê°€

### ë¡œê¹… ì„¤ì •
```python
# backend/app/core/config.pyì— ì¶”ê°€
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# AI ì„œë¹„ìŠ¤ ì „ìš© ë¡œê±°
logger = logging.getLogger('memento.ai')
```

---

*ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ êµ¬í˜„í•˜ë©´ ê¸°ë³¸ì ì¸ AI ì„œë¹„ìŠ¤ê°€ FastAPIì— í†µí•©ë©ë‹ˆë‹¤. ê° ë‹¨ê³„ë³„ë¡œ í…ŒìŠ¤íŠ¸í•˜ë©° ì ì§„ì ìœ¼ë¡œ êµ¬í˜„í•´ ë‚˜ê°€ì„¸ìš”.*