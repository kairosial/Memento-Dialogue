from typing import TypedDict, List, Dict, Any, Optional
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
import os
from supabase import create_client, Client
import uuid
from datetime import datetime

class WorkflowInput(TypedDict):
    """ê·¸ë˜í”„ ì‹¤í–‰ì„ ìœ„í•´ ì™¸ë¶€ì—ì„œ ì£¼ì…ë˜ëŠ” ì´ˆê¸° ë°ì´í„°"""
    conversation_id: str
    user_id: str
    user_message: str
    photo_context: Dict[str, Any]

class IntermediateState(TypedDict):
    """ë…¸ë“œ ê°„ ê²°ì •ì— ì‚¬ìš©ë˜ëŠ” ì„ì‹œ ë°ì´í„°"""
    cache_score: Optional[float]
    routing_decision: str

class FinalOutput(TypedDict):
    """ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ë  ê²°ê³¼ë¬¼"""
    response_text: str
    response_audio_url: Optional[str]

class GraphState(TypedDict):
    """ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€í†µí•˜ëŠ” ìƒíƒœ ê°ì²´"""
    input_data: WorkflowInput
    message_history: List[Dict[str, str]]
    intermediate: IntermediateState
    output: FinalOutput
    photo_info: Optional[Dict[str, Any]]  # ì‚¬ì§„ ì •ë³´ ì €ì¥
    session_id: Optional[str]  # ì„¸ì…˜ ID ì €ì¥
    _authenticated_client: Optional[Client]  # ì¸ì¦ëœ Supabase í´ë¼ì´ì–¸íŠ¸

class DialogueWorkflow:
    """LangGraph ê¸°ë°˜ ëŒ€í™” ì›Œí¬í”Œë¡œìš° ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
        openai_key = os.getenv("OPENAI_API_KEY")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_ANON_KEY")
        
        if not openai_key:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        if not supabase_url:
            raise ValueError("SUPABASE_URL environment variable is required")
        if not supabase_key:
            raise ValueError("SUPABASE_ANON_KEY environment variable is required")
        
        try:
            self.llm_mini = ChatOpenAI(
                model="gpt-5-mini",  # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½
                api_key=openai_key
            )
            self.llm_nano = ChatOpenAI(
                model="gpt-5-nano",  # ê²½ëŸ‰ ëª¨ë¸ë¡œ gpt-3.5-turbo ì‚¬ìš©
                max_tokens=256,
                api_key=openai_key
            )
            print("OpenAI LLM clients initialized successfully")
        except Exception as e:
            print(f"Failed to initialize OpenAI clients: {e}")
            raise
        
        try:
            # Supabase í´ë¼ì´ì–¸íŠ¸
            self.supabase: Client = create_client(
                supabase_url=supabase_url,
                supabase_key=supabase_key
            )
            print("Supabase client initialized successfully")
        except Exception as e:
            print(f"Failed to initialize Supabase client: {e}")
            raise
        
        try:
            # ì›Œí¬í”Œë¡œìš° êµ¬ì„±
            self.app = self._build_workflow()
            print("LangGraph workflow compiled successfully")
        except Exception as e:
            print(f"Failed to build LangGraph workflow: {e}")
            raise
    
    def _build_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(GraphState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("init_state", self.init_state_node)
        workflow.add_node("router", self.router_node)
        workflow.add_node("standard_response", self.standard_response_node)
        workflow.add_node("cache_retrieve", self.cache_retrieve_and_evaluate_node)
        workflow.add_node("fallback", self.fallback_node)
        
        # ì§„ì…ì  ì„¤ì •
        workflow.set_entry_point("init_state")
        
        # ì—£ì§€ ì •ì˜
        workflow.add_edge("init_state", "router")
        workflow.add_conditional_edges(
            "router",
            self._route_decision,
            {
                "standard_chat": "standard_response",
                "assessment_chat": "cache_retrieve"
            }
        )
        workflow.add_edge("standard_response", END)
        workflow.add_conditional_edges(
            "cache_retrieve",
            self._cache_decision,
            {
                "use_cache": END,
                "use_fallback": "fallback"
            }
        )
        workflow.add_edge("fallback", END)
        
        return workflow.compile()
    
    def init_state_node(self, state: GraphState) -> GraphState:
        """ìƒíƒœ ì´ˆê¸°í™” ë…¸ë“œ: DBì—ì„œ ëŒ€í™” ê¸°ë¡ ë° ì‚¬ì§„ ì •ë³´ ì¡°íšŒ"""
        conversation_id = state["input_data"]["conversation_id"]
        user_id = state["input_data"]["user_id"]
        photo_context = state["input_data"]["photo_context"]
        
        # ìƒíƒœì—ì„œ ì¸ì¦ëœ í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        client = state.get("_authenticated_client", self.supabase)
        
        print(f"ğŸ” ìƒíƒœ ì´ˆê¸°í™”: conversation_id={conversation_id}, user_id={user_id}")
        
        try:
            # ì‚¬ì§„ ì •ë³´ ì¡°íšŒ (photo_contextì— photo_idê°€ ìˆëŠ” ê²½ìš°)
            photo_info = None
            if photo_context.get("photo_id"):
                try:
                    photo_response = client.table("photos").select(
                        "id, filename, file_path, description, tags, location_name"
                    ).eq("id", photo_context["photo_id"]).single().execute()
                    
                    if photo_response.data:
                        photo_info = photo_response.data
                        print(f"ğŸ“· ì‚¬ì§„ ì •ë³´ ë¡œë“œë¨: {photo_info}")
                except Exception as photo_error:
                    print(f"âŒ ì‚¬ì§„ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {photo_error}")
            
            # conversation_idë¥¼ session_idë¡œ ì‚¬ìš© (main.pyì—ì„œ ì´ë¯¸ ì„¸ì…˜ ìƒì„±ë¨)
            session_id = conversation_id
            print(f"âœ… ì„¸ì…˜ ID ì„¤ì •: {session_id}")
            
            # í•´ë‹¹ ì„¸ì…˜ì˜ ê¸°ì¡´ ëŒ€í™” ë‚´ì—­ ì¡°íšŒ
            conversations_response = client.table("conversations").select(
                "id, question_text, user_response_text, conversation_order"
            ).eq("session_id", session_id).order("conversation_order").execute()
            
            print(f"ğŸ’¬ ê¸°ì¡´ ëŒ€í™” ë‚´ì—­: {len(conversations_response.data) if conversations_response.data else 0}ê°œ")
            
            # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ êµ¬ì„±
            system_content = "ë‹¹ì‹ ì€ ì¹˜ë§¤ ì§„ë‹¨ì„ ìœ„í•œ ë”°ëœ»í•œ ëŒ€í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."
            if photo_info:
                system_content += f" í˜„ì¬ ì‚¬ì§„ ì •ë³´: íŒŒì¼ëª…({photo_info.get('filename', 'N/A')}), ì„¤ëª…({photo_info.get('description', 'N/A')}), ìœ„ì¹˜({photo_info.get('location_name', 'N/A')}), íƒœê·¸({', '.join(photo_info.get('tags', []))})"
            
            message_history = [{"role": "system", "content": system_content}]
            
            # ê¸°ì¡´ ëŒ€í™” ë‚´ìš© ì¶”ê°€
            if conversations_response.data:
                for conv in conversations_response.data:
                    if conv.get("question_text"):
                        message_history.append({
                            "role": "assistant", 
                            "content": conv["question_text"]
                        })
                    if conv.get("user_response_text"):
                        message_history.append({
                            "role": "user", 
                            "content": conv["user_response_text"]
                        })
            
            state["message_history"] = message_history
            state["intermediate"] = {"cache_score": None, "routing_decision": ""}
            state["output"] = {"response_text": "", "response_audio_url": None}
            
            # photo_infoì™€ session_idë¥¼ ìƒíƒœì— ì €ì¥
            if photo_info:
                state["photo_info"] = photo_info
            state["session_id"] = session_id
            
        except Exception as e:
            print(f"Database operation failed: {e}")
            # ì—ëŸ¬ì‹œ ê¸°ë³¸ ìƒíƒœ ì„¤ì •
            system_content = "ë‹¹ì‹ ì€ ì¹˜ë§¤ ì§„ë‹¨ì„ ìœ„í•œ ë”°ëœ»í•œ ëŒ€í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."
            state["message_history"] = [{"role": "system", "content": system_content}]
            state["intermediate"] = {"cache_score": None, "routing_decision": ""}
            state["output"] = {"response_text": "", "response_audio_url": None}
            state["session_id"] = conversation_id
        
        return state
    
    def router_node(self, state: GraphState) -> GraphState:
        """ë¼ìš°í„° ë…¸ë“œ: ì¸ì§€ê¸°ëŠ¥ í‰ê°€ ì§ˆë¬¸ ì‚½ì… ì—¬ë¶€ ê²°ì •"""
        user_message = state["input_data"]["user_message"]
        message_history = state["message_history"]
        
        routing_prompt = f"""
        í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
        
        1. standard_chat: ì¼ë°˜ì ì¸ ì¼ìƒ ëŒ€í™” ì§„í–‰
        2. assessment_chat: ì¸ì§€ê¸°ëŠ¥ í‰ê°€ ì§ˆë¬¸ ì‚½ì…
        
        ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”:
        - ì‚¬ìš©ìê°€ ê¸°ì–µë ¥, ì‹œê°„, ì¥ì†Œì— ëŒ€í•´ ì–¸ê¸‰í•˜ê±°ë‚˜ í˜¼ë€ì„ ë³´ì´ë©´ â†’ assessment_chat
        - ì¼ë°˜ì ì¸ ì‚¬ì§„ ì„¤ëª…ì´ë‚˜ ì¼ìƒ ëŒ€í™”ì´ë©´ â†’ standard_chat
        
        ì‘ë‹µì€ ë°˜ë“œì‹œ 'standard_chat' ë˜ëŠ” 'assessment_chat' ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•˜ì„¸ìš”.
        """
        
        try:
            response = self.llm_mini.invoke([
                SystemMessage(content=routing_prompt),
                HumanMessage(content=user_message)
            ])
            
            routing_decision = response.content.strip().lower()
            if routing_decision not in ["standard_chat", "assessment_chat"]:
                routing_decision = "standard_chat"  # ê¸°ë³¸ê°’
                
            state["intermediate"]["routing_decision"] = routing_decision
            
        except Exception as e:
            print(f"Router decision failed: {e}")
            state["intermediate"]["routing_decision"] = "standard_chat"
        
        return state
    
    def standard_response_node(self, state: GraphState) -> GraphState:
        """ì¼ë°˜ ì‘ë‹µ ìƒì„± ë…¸ë“œ: ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ ëŒ€í™”"""
        user_message = state["input_data"]["user_message"]
        photo_context = state["input_data"]["photo_context"]
        photo_info = state.get("photo_info", {})
        
        # ì‚¬ì§„ ì •ë³´ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        photo_description = ""
        if photo_info:
            photo_description = f"ì‚¬ì§„ ì •ë³´: {photo_info.get('description', '')}, ìœ„ì¹˜: {photo_info.get('location_name', '')}, íƒœê·¸: {', '.join(photo_info.get('tags', []))}"
        
        conversation_prompt = f"""
        ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”.
        
        ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}
        {photo_description}
        
        ì‘ë‹µ ì›ì¹™:
        1. 50ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
        2. ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ ì–´ì¡°
        3. ì‚¬ì§„ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰
        4. ì¶”ê°€ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™” ì´ì–´ê°€ê¸°
        
        í•œ ë²ˆì— í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.llm_mini.invoke([
                SystemMessage(content=conversation_prompt),
                HumanMessage(content=user_message)
            ])
            
            state["output"]["response_text"] = response.content.strip()
            
        except Exception as e:
            print(f"Standard response generation failed: {e}")
            state["output"]["response_text"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        
        return state
    
    def cache_retrieve_and_evaluate_node(self, state: GraphState) -> GraphState:
        """ìºì‹œ ê²€ìƒ‰ ë° í‰ê°€ ë…¸ë“œ: ì¸ì§€ê¸°ëŠ¥ í‰ê°€ ì§ˆë¬¸ ê²€ìƒ‰"""
        user_message = state["input_data"]["user_message"]
        
        try:
            # Supabaseì—ì„œ CIST ì§ˆë¬¸ í…œí”Œë¦¿ ê²€ìƒ‰
            response = self.supabase.table("cist_question_templates").select(
                "*"
            ).limit(5).execute()
            
            if response.data:
                # ê°„ë‹¨í•œ ìœ ì‚¬ë„ í‰ê°€ (ì‹¤ì œë¡œëŠ” ë²¡í„° DB ì‚¬ìš© ê¶Œì¥)
                best_question = response.data[0]
                cache_score = 0.9  # ì„ì‹œ ì ìˆ˜
                
                state["intermediate"]["cache_score"] = cache_score
                state["output"]["response_text"] = best_question["template_text"]
            else:
                state["intermediate"]["cache_score"] = 0.3  # ë‚®ì€ ì ìˆ˜
                
        except Exception as e:
            print(f"Cache retrieval failed: {e}")
            state["intermediate"]["cache_score"] = 0.3
        
        return state
    
    def fallback_node(self, state: GraphState) -> GraphState:
        """ëŒ€ì²´ ì‘ë‹µ ì²˜ë¦¬ ë…¸ë“œ: ê²½ëŸ‰ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        user_message = state["input_data"]["user_message"]
        conversation_id = state["input_data"]["conversation_id"]
        photo_context = state["input_data"]["photo_context"]
        
        fallback_prompt = f"""
        ê°„ë‹¨í•˜ê³  ë”°ëœ»í•œ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.
        
        ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}
        
        30ì ì´ë‚´ë¡œ ê³µê°í•˜ë©° ë‹µë³€í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.llm_nano.invoke([
                SystemMessage(content=fallback_prompt),
                HumanMessage(content=user_message)
            ])
            
            state["output"]["response_text"] = response.content.strip()
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³ í’ˆì§ˆ ì§ˆë¬¸ ìƒì„± ìš”ì²­
            self._schedule_background_task(user_message, conversation_id, photo_context)
            
        except Exception as e:
            print(f"Fallback response failed: {e}")
            state["output"]["response_text"] = "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤."
        
        return state
    
    def _schedule_background_task(self, user_message: str, conversation_id: str, photo_context: dict):
        """Celeryë¥¼ í†µí•œ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìŠ¤ì¼€ì¤„ë§"""
        try:
            from tasks import generate_high_quality_questions
            
            context = {
                "user_message": user_message,
                "conversation_id": conversation_id, 
                "photo_context": photo_context
            }
            
            # ë¹„ë™ê¸° ì‘ì—… ë°œí–‰
            generate_high_quality_questions.delay(context)
            print(f"Background task scheduled for conversation: {conversation_id}")
            
        except Exception as e:
            print(f"Failed to schedule background task: {e}")
    
    def _route_decision(self, state: GraphState) -> str:
        """ë¼ìš°í„° ê²°ì •ì— ë”°ë¥¸ ê²½ë¡œ ì„ íƒ"""
        return state["intermediate"]["routing_decision"]
    
    def _cache_decision(self, state: GraphState) -> str:
        """ìºì‹œ ì ìˆ˜ì— ë”°ë¥¸ ê²½ë¡œ ì„ íƒ"""
        cache_score = state["intermediate"]["cache_score"]
        if cache_score and cache_score >= 0.85:
            return "use_cache"
        return "use_fallback"
    
    async def _save_conversation_to_db(self, state: GraphState, authenticated_client: Client = None) -> None:
        """ëŒ€í™” ë‚´ìš©ì„ DBì— ì €ì¥"""
        try:
            # ì¸ì¦ëœ í´ë¼ì´ì–¸íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
            client = authenticated_client if authenticated_client else self.supabase
            
            session_id = state.get("session_id")
            user_message = state["input_data"]["user_message"]
            ai_response = state["output"]["response_text"]
            photo_context = state["input_data"]["photo_context"]
            user_id = state["input_data"]["user_id"]
            
            print(f"ğŸ’¾ ëŒ€í™” ì €ì¥ ì‹œë„: session_id={session_id}, user_id={user_id}")
            
            if not session_id:
                print("âŒ session_id ì—†ìŒ, ëŒ€í™” ì €ì¥ ê±´ë„ˆëœ€")
                return
            
            # ë‹¤ìŒ conversation_order ê³„ì‚°
            count_response = client.table("conversations").select(
                "conversation_order"
            ).eq("session_id", session_id).execute()
            
            next_order = len(count_response.data) + 1 if count_response.data else 1
            print(f"ğŸ“Š ëŒ€í™” ìˆœì„œ: {next_order}")
            
            # ëŒ€í™” ë ˆì½”ë“œ ìƒì„±
            conversation_data = {
                "session_id": session_id,
                "user_id": user_id,
                "photo_id": photo_context.get("photo_id"),
                "conversation_order": next_order,
                "question_text": ai_response,
                "question_type": "open_ended",  # ê¸°ë³¸ê°’
                "user_response_text": user_message,
                "is_cist_item": False
            }
            
            print(f"ğŸ“ ëŒ€í™” ë°ì´í„°: {conversation_data}")
            
            insert_response = client.table("conversations").insert(conversation_data).execute()
            if insert_response.data:
                print(f"âœ… ëŒ€í™” ì €ì¥ ì„±ê³µ: {insert_response.data[0]['id']}")
            else:
                print("âŒ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: ì‘ë‹µ ë°ì´í„° ì—†ìŒ")
                
        except Exception as e:
            print(f"âŒ ëŒ€í™” ì €ì¥ DB ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            # ë””ë²„ê¹…ì„ ìœ„í•´ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
            import traceback
            print(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

    async def process_message(self, input_data: WorkflowInput, authenticated_client: Client = None) -> FinalOutput:
        """ë©”ì‹œì§€ ì²˜ë¦¬ ì§„ì…ì """
        initial_state = {
            "input_data": input_data,
            "message_history": [],
            "intermediate": {"cache_score": None, "routing_decision": ""},
            "output": {"response_text": "", "response_audio_url": None},
            "photo_info": None,
            "session_id": None,
            "_authenticated_client": authenticated_client
        }
        
        try:
            print(f"ğŸš€ ì›Œí¬í”Œë¡œìš° ì‹œì‘: conversation_id={input_data['conversation_id']}")
            
            final_state = await self.app.ainvoke(initial_state)
            print(f"âœ… ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: conversation_id={input_data['conversation_id']}")
            
            # ëŒ€í™” ë‚´ìš©ì„ DBì— ì €ì¥
            if final_state["output"]["response_text"]:
                try:
                    await self._save_conversation_to_db(final_state, authenticated_client)
                    print("âœ… ëŒ€í™” DB ì €ì¥ ì™„ë£Œ")
                except Exception as db_error:
                    print(f"âŒ ëŒ€í™” DB ì €ì¥ ì‹¤íŒ¨: {db_error}")
                    # ëŒ€í™” ì €ì¥ ì‹¤íŒ¨í•´ë„ ì‘ë‹µì€ ì „ì†¡
            
            return final_state["output"]
        except Exception as e:
            import traceback
            print(f"âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: conversation_id={input_data['conversation_id']}, error={e}")
            print(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                "response_text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "response_audio_url": None
            }