from typing import TypedDict, List, Dict, Any, Optional
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
import os
from supabase import create_client, Client
import uuid
from datetime import datetime
import sys

# ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))
from app.core.config import settings

class WorkflowInput(TypedDict):
    """ê·¸ë˜í”„ ì‹¤í–‰ì„ ìœ„í•´ ì™¸ë¶€ì—ì„œ ì£¼ì…ë˜ëŠ” ì´ˆê¸° ë°ì´í„°"""
    conversation_id: str
    user_id: str
    user_message: str
    photo_context: Dict[str, Any]

class IntermediateState(TypedDict):
    """ë…¸ë“œ ê°„ ê²°ì •ì— ì‚¬ìš©ë˜ëŠ” ì„ì‹œ ë°ì´í„°"""
    cache_score: Optional[float]
    routing_decision: str

class FinalOutput(TypedDict):
    """ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ë  ê²°ê³¼ë¬¼"""
    response_text: str
    response_audio_url: Optional[str]

class GraphState(TypedDict):
    """ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€í†µí•˜ëŠ” ìƒíƒœ ê°ì²´"""
    input_data: WorkflowInput
    message_history: List[Dict[str, str]]
    intermediate: IntermediateState
    output: FinalOutput
    execution_flow: List[str]  # ì‹¤í–‰ í”Œë¡œìš° ì¶”ì ìš©

class EnhancedDialogueWorkflow:
    """í…ŒìŠ¤íŠ¸ìš© í™•ì¥ëœ ëŒ€í™” ì›Œí¬í”Œë¡œìš° - ì‹¤í–‰ í”Œë¡œìš° ì¶”ì  ê¸°ëŠ¥ ì¶”ê°€"""
    
    def __init__(self):
        self.llm_mini = ChatOpenAI(
            model="gpt-4o-mini",  # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë³€ê²½
            api_key=settings.OPENAI_API_KEY
        )
        self.llm_nano = ChatOpenAI(
            model="gpt-4o-mini",  # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë³€ê²½
            temperature=0.5,
            max_tokens=256,
            api_key=settings.OPENAI_API_KEY
        )
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ (í…ŒìŠ¤íŠ¸ìš©ì´ë¯€ë¡œ ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
        try:
            self.supabase: Client = create_client(
                supabase_url=settings.SUPABASE_URL,
                supabase_key=settings.SUPABASE_ANON_KEY
            )
        except Exception as e:
            print(f"âš ï¸  Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
            self.supabase = None
        
        # ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.app = self._build_workflow()
    
    def _log_node_execution(self, node_name: str, state: GraphState) -> GraphState:
        """ë…¸ë“œ ì‹¤í–‰ ë¡œê·¸ ì¶”ê°€"""
        if "execution_flow" not in state:
            state["execution_flow"] = []
        
        state["execution_flow"].append(node_name)
        print(f"ğŸ”„ ë…¸ë“œ ì‹¤í–‰: {node_name}")
        
        # ìƒíƒœ ì •ë³´ ì¶œë ¥
        if node_name == "router":
            print(f"   ğŸ“ ì‚¬ìš©ì ë©”ì‹œì§€: {state['input_data']['user_message']}")
        elif node_name in ["cache_retrieve", "fallback"]:
            if state.get("intermediate", {}).get("cache_score"):
                print(f"   ğŸ“Š ìºì‹œ ì ìˆ˜: {state['intermediate']['cache_score']}")
        
        return state
    
    def _build_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(GraphState)
        
        # ë…¸ë“œ ì¶”ê°€ (ë¡œê¹…ì´ í¬í•¨ëœ ë˜í¼ í•¨ìˆ˜ë“¤)
        workflow.add_node("init_state", self._wrapped_init_state)
        workflow.add_node("router", self._wrapped_router)
        workflow.add_node("standard_response", self._wrapped_standard_response)
        workflow.add_node("cache_retrieve", self._wrapped_cache_retrieve)
        workflow.add_node("fallback", self._wrapped_fallback)
        
        # ì§„ì…ì  ì„¤ì •
        workflow.set_entry_point("init_state")
        
        # ì—£ì§€ ì •ì˜
        workflow.add_edge("init_state", "router")
        workflow.add_conditional_edges(
            "router",
            self._route_decision,
            {
                "standard_chat": "standard_response",
                "assessment_chat": "cache_retrieve"
            }
        )
        workflow.add_edge("standard_response", END)
        workflow.add_conditional_edges(
            "cache_retrieve",
            self._cache_decision,
            {
                "use_cache": END,
                "use_fallback": "fallback"
            }
        )
        workflow.add_edge("fallback", END)
        
        return workflow.compile()
    
    def _wrapped_init_state(self, state: GraphState) -> GraphState:
        """ë¡œê¹…ì´ í¬í•¨ëœ ìƒíƒœ ì´ˆê¸°í™” ë…¸ë“œ"""
        state = self._log_node_execution("init_state", state)
        return self.init_state_node(state)
    
    def _wrapped_router(self, state: GraphState) -> GraphState:
        """ë¡œê¹…ì´ í¬í•¨ëœ ë¼ìš°í„° ë…¸ë“œ"""
        state = self._log_node_execution("router", state)
        result = self.router_node(state)
        print(f"   ğŸ›¤ï¸  ë¼ìš°íŒ… ê²°ì •: {result['intermediate']['routing_decision']}")
        return result
    
    def _wrapped_standard_response(self, state: GraphState) -> GraphState:
        """ë¡œê¹…ì´ í¬í•¨ëœ ì¼ë°˜ ì‘ë‹µ ë…¸ë“œ"""
        state = self._log_node_execution("standard_response", state)
        result = self.standard_response_node(state)
        print(f"   ğŸ’¬ ì¼ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        return result
    
    def _wrapped_cache_retrieve(self, state: GraphState) -> GraphState:
        """ë¡œê¹…ì´ í¬í•¨ëœ ìºì‹œ ê²€ìƒ‰ ë…¸ë“œ"""
        state = self._log_node_execution("cache_retrieve", state)
        result = self.cache_retrieve_and_evaluate_node(state)
        print(f"   ğŸ” ìºì‹œ ê²€ìƒ‰ ì™„ë£Œ - ì ìˆ˜: {result['intermediate']['cache_score']}")
        return result
    
    def _wrapped_fallback(self, state: GraphState) -> GraphState:
        """ë¡œê¹…ì´ í¬í•¨ëœ ëŒ€ì²´ ì‘ë‹µ ë…¸ë“œ"""
        state = self._log_node_execution("fallback", state)
        result = self.fallback_node(state)
        print(f"   ğŸ”„ ëŒ€ì²´ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        return result
    
    # ê¸°ì¡´ ë…¸ë“œë“¤ì˜ êµ¬í˜„ (ì›ë³¸ê³¼ ë™ì¼í•˜ì§€ë§Œ ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
    def init_state_node(self, state: GraphState) -> GraphState:
        """ìƒíƒœ ì´ˆê¸°í™” ë…¸ë“œ: DBì—ì„œ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
        conversation_id = state["input_data"]["conversation_id"]
        
        try:
            if self.supabase:
                response = self.supabase.table("conversations").select(
                    "*, sessions(*)"
                ).eq("id", conversation_id).execute()
                
                if response.data:
                    conversation = response.data[0]
                    message_history = [
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹˜ë§¤ ì§„ë‹¨ì„ ìœ„í•œ ëŒ€í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."}
                    ]
                    
                    if conversation.get("ai_analysis"):
                        message_history.append({
                            "role": "assistant", 
                            "content": conversation.get("question_text", "")
                        })
                else:
                    message_history = [
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹˜ë§¤ ì§„ë‹¨ì„ ìœ„í•œ ëŒ€í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."}
                    ]
            else:
                # Supabase ì—°ê²°ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                message_history = [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹˜ë§¤ ì§„ë‹¨ì„ ìœ„í•œ ëŒ€í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."}
                ]
            
            state["message_history"] = message_history
            state["intermediate"] = {"cache_score": None, "routing_decision": ""}
            state["output"] = {"response_text": "", "response_audio_url": None}
            
        except Exception as e:
            print(f"âŒ Database query failed: {e}")
            state["message_history"] = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹˜ë§¤ ì§„ë‹¨ì„ ìœ„í•œ ëŒ€í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."}
            ]
            state["intermediate"] = {"cache_score": None, "routing_decision": ""}
            state["output"] = {"response_text": "", "response_audio_url": None}
        
        return state
    
    def router_node(self, state: GraphState) -> GraphState:
        """ë¼ìš°í„° ë…¸ë“œ: ì¸ì§€ê¸°ëŠ¥ í‰ê°€ ì§ˆë¬¸ ì‚½ì… ì—¬ë¶€ ê²°ì •"""
        user_message = state["input_data"]["user_message"]
        
        routing_prompt = f"""
        í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
        
        1. standard_chat: ì¼ë°˜ì ì¸ ì¼ìƒ ëŒ€í™” ì§„í–‰
        2. assessment_chat: ì¸ì§€ê¸°ëŠ¥ í‰ê°€ ì§ˆë¬¸ ì‚½ì…
        
        ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”:
        - ì‚¬ìš©ìê°€ ê¸°ì–µë ¥, ì‹œê°„, ì¥ì†Œì— ëŒ€í•´ ì–¸ê¸‰í•˜ê±°ë‚˜ í˜¼ë€ì„ ë³´ì´ë©´ â†’ assessment_chat
        - ì¼ë°˜ì ì¸ ì‚¬ì§„ ì„¤ëª…ì´ë‚˜ ì¼ìƒ ëŒ€í™”ì´ë©´ â†’ standard_chat
        
        ì‘ë‹µì€ ë°˜ë“œì‹œ 'standard_chat' ë˜ëŠ” 'assessment_chat' ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•˜ì„¸ìš”.
        """
        
        try:
            response = self.llm_mini.invoke([
                SystemMessage(content=routing_prompt),
                HumanMessage(content=user_message)
            ])
            
            routing_decision = response.content.strip().lower()
            if routing_decision not in ["standard_chat", "assessment_chat"]:
                routing_decision = "standard_chat"
                
            state["intermediate"]["routing_decision"] = routing_decision
            
        except Exception as e:
            print(f"âŒ Router decision failed: {e}")
            state["intermediate"]["routing_decision"] = "standard_chat"
        
        return state
    
    def standard_response_node(self, state: GraphState) -> GraphState:
        """ì¼ë°˜ ì‘ë‹µ ìƒì„± ë…¸ë“œ: ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ ëŒ€í™”"""
        user_message = state["input_data"]["user_message"]
        photo_context = state["input_data"]["photo_context"]
        
        conversation_prompt = f"""
        ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”.
        
        ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}
        ì‚¬ì§„ ì •ë³´: {photo_context}
        
        ì‘ë‹µ ì›ì¹™:
        1. 50ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
        2. ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ ì–´ì¡°
        3. ì‚¬ì§„ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰
        4. ì¶”ê°€ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™” ì´ì–´ê°€ê¸°
        
        í•œ ë²ˆì— í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.llm_mini.invoke([
                SystemMessage(content=conversation_prompt),
                HumanMessage(content=user_message)
            ])
            
            state["output"]["response_text"] = response.content.strip()
            
        except Exception as e:
            print(f"âŒ Standard response generation failed: {e}")
            state["output"]["response_text"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        
        return state
    
    def cache_retrieve_and_evaluate_node(self, state: GraphState) -> GraphState:
        """ìºì‹œ ê²€ìƒ‰ ë° í‰ê°€ ë…¸ë“œ: ì¸ì§€ê¸°ëŠ¥ í‰ê°€ ì§ˆë¬¸ ê²€ìƒ‰"""
        user_message = state["input_data"]["user_message"]
        
        try:
            if self.supabase:
                response = self.supabase.table("cist_question_templates").select(
                    "*"
                ).limit(5).execute()
                
                if response.data:
                    best_question = response.data[0]
                    cache_score = 0.9
                    
                    state["intermediate"]["cache_score"] = cache_score
                    state["output"]["response_text"] = best_question["template_text"]
                else:
                    state["intermediate"]["cache_score"] = 0.3
                    state["output"]["response_text"] = "ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ë§ì”€í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?"
            else:
                # Supabase ì—°ê²°ì´ ì—†ëŠ” ê²½ìš° Mock ë°ì´í„°
                state["intermediate"]["cache_score"] = 0.9
                state["output"]["response_text"] = "ì˜¤ëŠ˜ì´ ë©°ì¹ ì¸ì§€ ê¸°ì–µí•˜ì‹œë‚˜ìš”?"
                
        except Exception as e:
            print(f"âŒ Cache retrieval failed: {e}")
            state["intermediate"]["cache_score"] = 0.3
            state["output"]["response_text"] = "ê¸°ì–µì— ê´€í•´ ì¡°ê¸ˆ ë” ì´ì•¼ê¸°í•´ì£¼ì‹¤ê¹Œìš”?"
        
        return state
    
    def fallback_node(self, state: GraphState) -> GraphState:
        """ëŒ€ì²´ ì‘ë‹µ ì²˜ë¦¬ ë…¸ë“œ: ê²½ëŸ‰ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        user_message = state["input_data"]["user_message"]
        
        fallback_prompt = f"""
        ê°„ë‹¨í•˜ê³  ë”°ëœ»í•œ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.
        
        ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}
        
        30ì ì´ë‚´ë¡œ ê³µê°í•˜ë©° ë‹µë³€í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.llm_nano.invoke([
                SystemMessage(content=fallback_prompt),
                HumanMessage(content=user_message)
            ])
            
            state["output"]["response_text"] = response.content.strip()
            
        except Exception as e:
            print(f"âŒ Fallback response failed: {e}")
            state["output"]["response_text"] = "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤."
        
        return state
    
    def _route_decision(self, state: GraphState) -> str:
        """ë¼ìš°í„° ê²°ì •ì— ë”°ë¥¸ ê²½ë¡œ ì„ íƒ"""
        decision = state["intermediate"]["routing_decision"]
        print(f"   ğŸ¯ ë¼ìš°íŒ… ê²½ë¡œ: {decision}")
        return decision
    
    def _cache_decision(self, state: GraphState) -> str:
        """ìºì‹œ ì ìˆ˜ì— ë”°ë¥¸ ê²½ë¡œ ì„ íƒ"""
        cache_score = state["intermediate"]["cache_score"]
        if cache_score and cache_score >= 0.85:
            decision = "use_cache"
        else:
            decision = "use_fallback"
        
        print(f"   ğŸ“ˆ ìºì‹œ ê²°ì •: {decision} (ì ìˆ˜: {cache_score})")
        return decision
    
    async def process_message(self, input_data: WorkflowInput) -> Dict[str, Any]:
        """ë©”ì‹œì§€ ì²˜ë¦¬ ì§„ì…ì  - ì‹¤í–‰ í”Œë¡œìš° ì •ë³´ í¬í•¨"""
        initial_state = {
            "input_data": input_data,
            "message_history": [],
            "intermediate": {"cache_score": None, "routing_decision": ""},
            "output": {"response_text": "", "response_audio_url": None},
            "execution_flow": []
        }
        
        print(f"\nğŸš€ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {input_data['user_message']}")
        print("=" * 60)
        
        try:
            final_state = await self.app.ainvoke(initial_state)
            
            print("=" * 60)
            print(f"âœ… ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!")
            print(f"ğŸ”„ ì‹¤í–‰ëœ ë…¸ë“œë“¤: {' â†’ '.join(final_state['execution_flow'])}")
            print(f"ğŸ’¬ ìµœì¢… ì‘ë‹µ: {final_state['output']['response_text']}")
            
            return {
                "response_text": final_state["output"]["response_text"],
                "response_audio_url": final_state["output"]["response_audio_url"],
                "execution_flow": final_state["execution_flow"]
            }
            
        except Exception as e:
            print(f"âŒ Workflow execution failed: {e}")
            return {
                "response_text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "response_audio_url": None,
                "execution_flow": ["error"]
            }


async def test_standard_chat():
    """ì¼ë°˜ ëŒ€í™” í…ŒìŠ¤íŠ¸"""
    workflow = EnhancedDialogueWorkflow()
    
    test_input: WorkflowInput = {
        "conversation_id": str(uuid.uuid4()),
        "user_id": "test-user-123",
        "user_message": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
        "photo_context": {"description": "ë°ì€ í•˜ëŠ˜ ì‚¬ì§„", "objects": ["í•˜ëŠ˜", "êµ¬ë¦„"]}
    }
    
    print("\n" + "="*80)
    print("ğŸ§ª ì¼ë°˜ ëŒ€í™” í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    try:
        result = await workflow.process_message(test_input)
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   ì‘ë‹µ: {result['response_text']}")
        print(f"   ì‹¤í–‰ í”Œë¡œìš°: {' â†’ '.join(result['execution_flow'])}")
        print("   âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True
    except Exception as e:
        print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


async def test_assessment_chat():
    """í‰ê°€ ëŒ€í™” í…ŒìŠ¤íŠ¸"""
    workflow = EnhancedDialogueWorkflow()
    
    test_input: WorkflowInput = {
        "conversation_id": str(uuid.uuid4()),
        "user_id": "test-user-124", 
        "user_message": "ì–´ì œê°€ ëª‡ ì¼ì´ì—ˆëŠ”ì§€ ê¸°ì–µì´ ì•ˆ ë‚˜ìš”.",
        "photo_context": {"description": "ì¼ê¸°ì¥ ì‚¬ì§„", "objects": ["ì¼ê¸°ì¥", "íœ"]}
    }
    
    print("\n" + "="*80)
    print("ğŸ§ª í‰ê°€ ëŒ€í™” í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    try:
        result = await workflow.process_message(test_input)
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   ì‘ë‹µ: {result['response_text']}")
        print(f"   ì‹¤í–‰ í”Œë¡œìš°: {' â†’ '.join(result['execution_flow'])}")
        print("   âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True
    except Exception as e:
        print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


async def test_edge_cases():
    """ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    workflow = EnhancedDialogueWorkflow()
    
    # ì• ë§¤í•œ ê²½ê³„ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "name": "ê²½ê³„ ì¼€ì´ìŠ¤ 1: ì‹œê°„ ê´€ë ¨ ì¼ë°˜ ëŒ€í™”",
            "message": "ì˜¤ëŠ˜ ì‹œê°„ì´ ì°¸ ë¹¨ë¦¬ ê°€ë„¤ìš”",
            "expected_route": "standard_chat"
        },
        {
            "name": "ê²½ê³„ ì¼€ì´ìŠ¤ 2: ëª…í™•í•œ ê¸°ì–µë ¥ ë¬¸ì œ",
            "message": "ë°©ê¸ˆ ë­ë¼ê³  í–ˆëŠ”ì§€ ê¸°ì–µì´ ì•ˆë‚˜ìš”",
            "expected_route": "assessment_chat"
        },
        {
            "name": "ê²½ê³„ ì¼€ì´ìŠ¤ 3: ë¹ˆ ë©”ì‹œì§€",
            "message": "",
            "expected_route": "standard_chat"
        }
    ]
    
    results = []
    for i, test_case in enumerate(test_cases):
        print(f"\n" + "="*80)
        print(f"ğŸ§ª {test_case['name']}")
        print("="*80)
        
        test_input: WorkflowInput = {
            "conversation_id": str(uuid.uuid4()),
            "user_id": f"test-user-{125+i}",
            "user_message": test_case["message"],
            "photo_context": {"description": "í…ŒìŠ¤íŠ¸ ì‚¬ì§„", "objects": ["í…ŒìŠ¤íŠ¸"]}
        }
        
        try:
            result = await workflow.process_message(test_input)
            route_taken = "standard_chat" if "standard_response" in result["execution_flow"] else "assessment_chat"
            
            print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            print(f"   ì…ë ¥: '{test_case['message']}'")
            print(f"   ì˜ˆìƒ ê²½ë¡œ: {test_case['expected_route']}")
            print(f"   ì‹¤ì œ ê²½ë¡œ: {route_taken}")
            print(f"   ì‹¤í–‰ í”Œë¡œìš°: {' â†’ '.join(result['execution_flow'])}")
            print(f"   ì‘ë‹µ: {result['response_text']}")
            
            success = route_taken == test_case["expected_route"]
            print(f"   {'âœ… ì˜ˆìƒëŒ€ë¡œ ë¼ìš°íŒ…ë¨' if success else 'âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ë¼ìš°íŒ…'}")
            results.append(success)
            
        except Exception as e:
            print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results.append(False)
    
    return results


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ”¬ Enhanced DialogueWorkflow í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("\nì´ í…ŒìŠ¤íŠ¸ëŠ” ê° ë…¸ë“œì˜ ì‹¤í–‰ ê³¼ì •ê³¼ íë¦„ì„ ìƒì„¸íˆ ë³´ì—¬ì¤ë‹ˆë‹¤.")
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    if not settings.OPENAI_API_KEY:
        print("\nâŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("backend/.env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print("\ní…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ Mock ì‘ë‹µìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results = []
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë“¤
    test_results.append(await test_standard_chat())
    test_results.append(await test_assessment_chat())
    
    # ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    edge_results = await test_edge_cases()
    test_results.extend(edge_results)
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    passed = sum(test_results)
    total = len(test_results)
    
    print(f"âœ… ì„±ê³µ: {passed}/{total}")
    print(f"âŒ ì‹¤íŒ¨: {total-passed}/{total}")
    
    if passed == total:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâš ï¸ {total-passed}ê°œì˜ í…ŒìŠ¤íŠ¸ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    print("\nğŸ’¡ ì´ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ë‹¤ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print("   - ê° ë…¸ë“œê°€ ì‹¤í–‰ë˜ëŠ” ìˆœì„œ")
    print("   - ë¼ìš°íŒ… ê²°ì • ê³¼ì •") 
    print("   - ìºì‹œ ì ìˆ˜ì™€ ëŒ€ì²´ ë¡œì§ ì„ íƒ")
    print("   - ì—ëŸ¬ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())